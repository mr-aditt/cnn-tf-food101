{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNCAdiyFk9PC"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg1H391Xyj-t"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle\n",
        "from pathlib import Path\n",
        "import json\n",
        "import shutil\n",
        "from os import listdir\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as tfb\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "2iOJCwkHE7fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download and unzip dataset"
      ],
      "metadata": {
        "id": "YUDuKFggFVMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_extract_data():\n",
        "\n",
        "  if Path('food-101').exists():\n",
        "    print(\"Dataset already exists\")\n",
        "  else:\n",
        "    if not Path('food-101.tar.gz').exists():\n",
        "      print(\"Dataset does not exist\")\n",
        "      !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "    !tar xzvf food-101.tar.gz\n",
        "    print(\"Data Extracted\")"
      ],
      "metadata": {
        "id": "ErvzZu3bFTAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create train and test dirs"
      ],
      "metadata": {
        "id": "sedx0FmbFkAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to create train and test folders\n",
        "def create_train_test_dir(src, dst, classes_list, train_json, test_json):\n",
        "  if Path(dst).exists():\n",
        "    shutil.rmtree(dst)\n",
        "  print(f'{str(\"SET TYPE\").ljust(10)} {str(\"SOURCE\").ljust(40)} {str(\"DESTINATION\").ljust(40)}  {str(\"#FILES\").ljust(40)}')\n",
        "  for c_name in classes_mini:\n",
        "\n",
        "    train_sub_class_path = Path(dst).joinpath('train').joinpath(c_name)\n",
        "    test_sub_class_path = Path(dst).joinpath('test').joinpath(c_name)\n",
        "    \n",
        "    # create folder for each class\n",
        "    train_sub_class_path.mkdir(parents=True)\n",
        "    test_sub_class_path.mkdir(parents=True)\n",
        "\n",
        "    # Copy files to train\n",
        "    for f_path in train_json[c_name]:\n",
        "      shutil.copy(Path(src).joinpath(f_path+'.jpg'), train_sub_class_path)\n",
        "    print(f'{str(\"Train\").ljust(10)} {str(Path(src).joinpath(c_name)).ljust(40)} {str(train_sub_class_path).ljust(40)}  {len(listdir(train_sub_class_path))}')\n",
        "    # Copy files to test\n",
        "    for f_path in test_json[c_name]:\n",
        "      shutil.copy(Path(src).joinpath(f_path+'.jpg'), test_sub_class_path)\n",
        "    print(f'{str(\"Test\").ljust(10)} {str(Path(src).joinpath(c_name)).ljust(40)} {str(test_sub_class_path).ljust(40)}  {len(listdir(test_sub_class_path))}')\n",
        "\n"
      ],
      "metadata": {
        "id": "SzMN1Aw6FisS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the dataset"
      ],
      "metadata": {
        "id": "73nak2jlF1UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_random_images(dir_, class_list):\n",
        "  label = ''\n",
        "  fig, axes = plt.subplots((len(class_list)//6)+1, 6, figsize=(20,20))\n",
        "  for ax, c_name in zip(axes.ravel(), class_list):\n",
        "    image_path = random.sample(listdir(Path(dir_).joinpath(c_name)),1)[0] \n",
        "    ax.imshow(mpimg.imread(Path(dir_).joinpath(c_name).joinpath(image_path)))\n",
        "    ax.set_title(c_name.replace('_', ' ').capitalize())\n",
        "    ax.set_axis_off()\n",
        "  fig.delaxes(axes[16][5])\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "DdpBjyNHFzUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Loss Curves using Model's history callback"
      ],
      "metadata": {
        "id": "ialJmzq9FExE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(history):\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "L_-MOINzFAji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot loss curves between 2 models"
      ],
      "metadata": {
        "id": "3sarilKSnIGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_and_compare_loss_curves(history_list, model_names):\n",
        "   \n",
        "#   epochs = range(1, len(history.history['loss'])+1)\n",
        "#   fig, axes = plt.subplots(2,2,True,True,figsize=(20,5))\n",
        "#   for history in history_list:\n",
        "#   for ax in axes.ravel():\n",
        "#     ax.plot(epochs, history.history['loss'], label='Training Loss')\n",
        "#     ax.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
        "#     ax.set_title(c_name.replace('_', ' ').capitalize())\n",
        "#     ax.xlabel('Epochs')\n",
        "#     ax.ylabel('Loss')\n",
        "#   fig.show()\n"
      ],
      "metadata": {
        "id": "VdEiMDPNnQ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqckOFQlFTp"
      },
      "source": [
        "## Data\n",
        "\n",
        "- The [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) dataset has 101000 images of food with 101 classes.\n",
        "- Each class has 1000 samples divided into 750 training samples and 250 test samples.\n",
        "> 📝 **Note:** From Food-101 webpage:\n",
        "On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.\n",
        "The entire dataset is 5GB in size\n",
        "\n",
        "> 🔔 **Original Paper** [here](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Unzip data"
      ],
      "metadata": {
        "id": "OTsbD-SBC9tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu6QGu2GkTMq"
      },
      "outputs": [],
      "source": [
        "get_extract_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Train and Test Directories\n",
        "\n",
        "The downloaded data is in a single directory (namely **Food-101**), with sub-folder names as class name, which is fine if we only want to train the model. But usually it's not the case. So we'll create 2 directory under a dir **dataset** each with sub_dirs: ***lasagna, macaroni_and_cheese, pizza, caprese_salad, club_sandwich, dumplings, french_onion_soup, tacos, ramen, spaghetti_bolognese***\n",
        "\n",
        "The dir structure will be:\n",
        "```\n",
        "├── root\n",
        "│   ├── dataset\n",
        "│   │   ├── train\n",
        "|   │   |   ├── lasagna\n",
        "|   │   |   ├── macaroni_and_cheese\n",
        "|   │   |   ├── ...\n",
        "|   │   |\n",
        "|   |   ├── test\n",
        "|   │   |   ├── lasagna\n",
        "|   │   |   ├── macaroni_and_cheese\n",
        "|   │   |   ├── ...\n",
        "|   │   |\n",
        "|   |   ├── Food-101\n",
        "|   │   |   ├── images\n",
        "|   │   |   ├── meta\n",
        "|   │   |   ├── ...\n",
        "|   |   ├── food-101.tar.gz\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "U5tq_XIyDFF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting image file names, classes and labels\n",
        "\n",
        "These informations are stored under\n",
        "\n",
        "```Food-101 --> meta```\n",
        "\n",
        "The file:\n",
        "- labels.txt contains labels\n",
        "- classes.txt contains classes\n",
        "- train.json contains key, values pairs. keys = classes and values = image path\n",
        "- test.json contains key, values pairs. keys = classes and values = image path\n",
        "\n",
        "> 📝 **Note:** **```test.json```** contains 250 handpicked images (in each class) path. This is good for testing our model"
      ],
      "metadata": {
        "id": "8HteJASrDXeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbMMpcBl2k4f"
      },
      "outputs": [],
      "source": [
        "train_file = 'food-101/meta/train.json'\n",
        "test_file = 'food-101/meta/test.json'\n",
        "classes_file = 'food-101/meta/classes.txt'\n",
        "labels_file = 'food-101/meta/labels.txt'\n",
        "\n",
        "\n",
        "with open(\"food-101/meta/train.json\", \"r\") as f:\n",
        "    train_json = json.loads(f.read())\n",
        "with open(\"food-101/meta/test.json\", \"r\") as f:\n",
        "    test_json = json.loads(f.read())\n",
        "with open(\"food-101/meta/classes.txt\", \"r\") as f:\n",
        "    classes = f.read()\n",
        "    classes = classes.strip('\\n').split('\\n')\n",
        "with open(\"food-101/meta/labels.txt\", \"r\") as f:\n",
        "  labels = f.read()\n",
        "  labels = labels.strip('\\n').split('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahkXCxTQ5W-r"
      },
      "outputs": [],
      "source": [
        "# List sub-dirs of food-101/images with size\n",
        "total = 0\n",
        "print('DIRECTORY PATH'.ljust(50), 'SIZE')\n",
        "for sub_path in Path('food-101/images').iterdir():\n",
        "  total+=sub_path.stat().st_size\n",
        "  print(str(sub_path).ljust(50), str(round(sub_path.stat().st_size*0.001, 2))+\" kb\")\n",
        "print('\\n'+str('TOTAL').ljust(50), str(round(total*0.001, 2))+\" kb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to create train-test dirs"
      ],
      "metadata": {
        "id": "kjSf1SxHNi99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3x9wComQ99x"
      },
      "outputs": [],
      "source": [
        "# Initially, we usually don't train the model on huge dataset\n",
        "# because we want to understand how our model is performing on\n",
        "# subset of dataset. Then, ones the results are satisfactory,\n",
        "# we can up the model by providing the whole dataset.\n",
        "\n",
        "# Ergo, create mini classes\n",
        "classes_mini = [\n",
        "               'caprese_salad',\n",
        "               'club_sandwich',\n",
        "               'dumplings',\n",
        "               'french_onion_soup',\n",
        "               'lasagna',\n",
        "               'macaroni_and_cheese',\n",
        "               'pizza',\n",
        "               'ramen',\n",
        "               'spaghetti_bolognese',\n",
        "               'tacos',\n",
        "               ]\n",
        "\n",
        "\n",
        "\n",
        "src = 'food-101/images'\n",
        "dst = 'dataset'\n",
        "create_train_test_dir(src, dst, classes_mini, train_json, test_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize"
      ],
      "metadata": {
        "id": "Ic5wgqQOOD07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_random_images('food-101/images', classes)"
      ],
      "metadata": {
        "id": "6fXJc2UKUGCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5G_7zyZMwF2"
      },
      "outputs": [],
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels \n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQT6v6jAJJe"
      },
      "source": [
        "https://www.kaggle.com/srajanseth/food-vision-51/notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data for model\n",
        "\n",
        "Here we're perfoming **data augmentation** — increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data, using tensorflow's **ImageDataGenerator** class. Then we'll pass the generated images to tensorflow's **flow_from_directory** method which will take images as predictor and directory name as class (i.e. response)."
      ],
      "metadata": {
        "id": "3RdtBVz0AJ4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYIjgougEZ5s"
      },
      "outputs": [],
      "source": [
        "tfb.clear_session()\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = str(Path(Path.cwd()).joinpath('dataset').joinpath('train'))\n",
        "test_dir = str(Path(Path.cwd()).joinpath('dataset').joinpath('test'))\n",
        "\n",
        "# Preprocess data (scaling i.e. normalize)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        "    )\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "HEIGHT, WIDTH, N_COLOR = 224,224,3\n",
        "BATCH_SIZE = 256\n",
        "n_classes = 10\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=BATCH_SIZE, # number of images to process at a time \n",
        "                                               target_size=(HEIGHT, WIDTH), # convert all images to be 224 x 224\n",
        "                                               class_mode=\"categorical\", # type of problem we're working on\n",
        "                                               seed=42)\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               target_size=(HEIGHT, WIDTH),\n",
        "                                               class_mode=\"categorical\",\n",
        "                                               seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqPy1Ix-lPFm"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create and compile"
      ],
      "metadata": {
        "id": "rtSDWMjzDaeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VegL7GxKzMuS"
      },
      "outputs": [],
      "source": [
        "# Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/)\n",
        "baseline_model = tf.keras.Sequential([\n",
        "                                      InputLayer(\n",
        "                                          input_shape=(HEIGHT, WIDTH, N_COLOR),\n",
        "                                          # batch_size=BATCH_SIZE,\n",
        "                                          name='Input'\n",
        "                                          ),\n",
        "                                      Conv2D(\n",
        "                                          filters=10,\n",
        "                                          kernel_size=3,\n",
        "                                          activation='relu',\n",
        "                                          #  input_shape=(HEIGHT, WIDTH, N_COLOR),\n",
        "                                          name='CONV_1'\n",
        "                                          ),\n",
        "                                      Conv2D(\n",
        "                                          filters=10,\n",
        "                                          kernel_size=3,\n",
        "                                          activation='relu',\n",
        "                                          name='CONV_2'\n",
        "                                          ),\n",
        "                                      MaxPool2D(\n",
        "                                          pool_size=2,\n",
        "                                          name='POOL_1'\n",
        "                                          ),\n",
        "                                      Conv2D(\n",
        "                                          filters=10,\n",
        "                                          kernel_size=3,\n",
        "                                          activation='relu',\n",
        "                                          name='CONV_3'\n",
        "                                          ),\n",
        "                                      Conv2D(\n",
        "                                          filters=10,\n",
        "                                          kernel_size=3,\n",
        "                                          activation='relu',\n",
        "                                          name='CONV_4'\n",
        "                                          ),\n",
        "                                      MaxPool2D(\n",
        "                                          pool_size=2,\n",
        "                                          name='POOL_2'\n",
        "                                          ),\n",
        "                                      Flatten(),\n",
        "                                      Dense(\n",
        "                                          units=10,\n",
        "                                          activation='softmax',\n",
        "                                          name='Output'\n",
        "                                          )\n",
        "                                      ], name='Baseline Model')\n",
        "\n",
        "baseline_model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaw9tKJG1E0v"
      },
      "outputs": [],
      "source": [
        "# plot_model(baseline_model, show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit"
      ],
      "metadata": {
        "id": "Zek8fC7cDflV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTm1j9yvHJu9"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "baseline_history = baseline_model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    validation_data = test_data,\n",
        "    validation_steps=int(0.25*len(test_data)),\n",
        "    callbacks=[\n",
        "              EarlyStopping,\n",
        "    ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot loss curves"
      ],
      "metadata": {
        "id": "cjKi1KetDink"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xIan9dWckiX"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(baseline_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate"
      ],
      "metadata": {
        "id": "8qPkwFJyDn-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_eval = baseline_model.evaluate(test_data)\n",
        "print(f'{baseline_model.metrics_names[0]:<{8}} | {round(baseline_eval[0], 3)}%')\n",
        "print(f'{baseline_model.metrics_names[1]:<{8}} | {round(baseline_eval[1], 3)*100}%')"
      ],
      "metadata": {
        "id": "dksJMo_bDphc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "it59xfMTDq53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create & Compile"
      ],
      "metadata": {
        "id": "3d2LfKucEb3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                              InputLayer(\n",
        "                                  input_shape=(HEIGHT, WIDTH, N_COLOR),\n",
        "                                  # batch_size=BATCH_SIZE,\n",
        "                                  name='Input'\n",
        "                                  ),\n",
        "                              Conv2D(\n",
        "                                  filters=10,\n",
        "                                  kernel_size=3,\n",
        "                                  activation='relu',\n",
        "                                  #  input_shape=(HEIGHT, WIDTH, N_COLOR),\n",
        "                                  name='CONV_1'\n",
        "                                  ),\n",
        "                              Conv2D(\n",
        "                                  filters=10,\n",
        "                                  kernel_size=3,\n",
        "                                  activation='relu',\n",
        "                                  name='CONV_2'\n",
        "                                  ),\n",
        "                              MaxPool2D(\n",
        "                                  pool_size=2,\n",
        "                                  name='POOL_1'\n",
        "                                  ),\n",
        "                              Conv2D(\n",
        "                                  filters=10,\n",
        "                                  kernel_size=3,\n",
        "                                  activation='relu',\n",
        "                                  name='CONV_3'\n",
        "                                  ),\n",
        "                              Conv2D(\n",
        "                                  filters=10,\n",
        "                                  kernel_size=3,\n",
        "                                  strides=2,\n",
        "                                  activation='relu',\n",
        "                                  name='CONV_4'\n",
        "                                  ),\n",
        "                              MaxPool2D(\n",
        "                                  pool_size=2,\n",
        "                                  name='POOL_2'\n",
        "                                  ),\n",
        "                              Flatten(),\n",
        "                              Dense(\n",
        "                                  units=10,\n",
        "                                  activation='softmax',\n",
        "                                  name='Output'\n",
        "                                  )\n",
        "                              ], name='Model')\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "afss1SERDspw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit"
      ],
      "metadata": {
        "id": "JqIOyOBuEgq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_checkpoint_path = Path(Path.cwd()).joinpath('Model-checkpoint')\n",
        "model_checkpoint_path.mkdir(parents=True)\n",
        "\n",
        "# Callbacks\n",
        "EarlyStopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "ModelCheckpoint = ModelCheckpoint(model_checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=25,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    validation_data = test_data,\n",
        "    validation_steps=int(0.25*len(test_data)),\n",
        "    callbacks=[\n",
        "              EarlyStopping,\n",
        "    ]\n",
        "    )"
      ],
      "metadata": {
        "id": "7Io1zH0EEjct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot loss curves"
      ],
      "metadata": {
        "id": "lKw7XDNsEosk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history)"
      ],
      "metadata": {
        "id": "oWT6V0CMEqw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate"
      ],
      "metadata": {
        "id": "jKvyeYj6ExsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_eval = model.evaluate(test_data)\n",
        "print(f'{model.metrics_names[0]:<{8}} | {round(model_eval[0], 3)}%')\n",
        "print(f'{model.metrics_names[1]:<{8}} | {round(model_eval[1], 3)*100}%')"
      ],
      "metadata": {
        "id": "ekgcX7kEEzpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss curve: Baseline vs. Model"
      ],
      "metadata": {
        "id": "nF5hBD5LmZao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_and_compare_loss_curves(history)"
      ],
      "metadata": {
        "id": "9v-EqAY9mYU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2iOJCwkHE7fl",
        "YUDuKFggFVMe",
        "sedx0FmbFkAJ",
        "73nak2jlF1UI",
        "ialJmzq9FExE",
        "3sarilKSnIGt",
        "8HteJASrDXeX",
        "kjSf1SxHNi99",
        "Ic5wgqQOOD07",
        "nF5hBD5LmZao"
      ],
      "name": "CNN using Tf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}